{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c947e724-8ecb-4025-a426-ea542ce9ea5f",
   "metadata": {},
   "source": [
    "# UFO Project Report for Fish Keypoint Detection #\n",
    "\n",
    "Sam Nündel, for M.Sc. Information Engineering\n",
    "\n",
    "August 2024\n",
    "\n",
    "### Content ###\n",
    "\n",
    "1. Introduction & Related Work\n",
    "2. Implementation\n",
    "3. Experiments\n",
    "5. Results\n",
    "\n",
    "### Instructions on running this notebook ###\n",
    "\n",
    "* clone repository FishVision including the necessary, adapted YOLOv7 main and pose branches, as well as the most recent dataset\n",
    "* change paths to your own environment's\n",
    "* run the pip install and conda install commands for the necessary versions of certain packages that work best in combination with each other\n",
    "\n",
    "### Other repositories (own and used) ###\n",
    "\n",
    "https://github.com/fhkiel-mlaip/ufo-keypoint-detection\n",
    "https://github.com/WongKinYiu/yolov7\n",
    "https://github.com/ultralytics/JSON2YOLO/\n",
    "https://github.com/TanjaNuendel/FishVision (Code for Norwegian working group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e35a9f-4d59-4f37-b3ac-522208d64dfd",
   "metadata": {},
   "source": [
    "## Introduction & Related Work ##\n",
    "\n",
    "### Project Context ###\n",
    "\n",
    "The scope of this particular task in Project UFO was to examine and adapt various methods of keypoint detection to cod fish, filmed by an underwater observatory in the Baltic Sea. After having completed a similar course at Universitet i Agder in Norway, the results on salmon in aqua culturing were compiled and funneled into additional experiments, specific to the cod fish dataset.\n",
    "\n",
    "### Keypoint detection in aquatic animals ###\n",
    "\n",
    "The current state-of-the-art methods in aquatic monitoring consist of a large number of computer vision systems that work on a variety of different architectures and methods from simpler mathematical classifiers to highly complex detection pipelines using neural network architectures. It can be attested that the YOLO model family has been very popular for tasks relation to underwater image detection, especially with moving fish in real-time applications. In 2020, Mohamed et al. introduced MSR-YOLO [7], an adaptation of YOLO to specific fish detection, a model family that has often been used for aquatic monitoring or species detection, such as by Kalhagen et al. [8], who used a hierarchical approach with YOLOv3. The same network was used by Wang et al. in 2021 for general fish instance detection [9] Chuang et al 2020 used Mask-RCNN for segmentation, but did not mention specific keypoint-based measurement [10]. YOLO-Fish by Muksit et al. [11] deserves a mention as a further adaptation of a YOLO model to the sphere of fish detection. It can be concluded that the realm of fish detection and monitoring consists of a fairly large volume of research that branches into numerous areas of AI-based techniques.\n",
    "\n",
    "Usually, keypoint detection is applied with the purpose of pose estimation, especially for Current SOTA for keypoint detection (COCO Benchmark for Keypoint Detection is 4xRSN50 as of 2020) is the 4xRSN-50 network as proposed by [12]. Reaching a test AP of 78.6 %, it is yet to be adapted to\n",
    "other purposes than human pose estimation, but remains the current winner. A Mask-RCNN [13] implementation called Keypoint RCNN which only exists in a PyTorch implementation, consists of masks for single keypoints being used for anatomical keypoint analysis. For this research context, pose estimation is currently not of the greatest interest, but could be used later in adapting a monitoring system for behavioral analysis using fish poses and tracking movement.\n",
    "\n",
    "Species-specific keypoint detection in fish to obtain either measurements or predict animal pose has not been a focus topic both in aquatic science resarch, as well as computer vision in general. Yet, Suo et al. 2020 [14] and Chen et al. 2017 [15] used two different types of neural network architectures for fish keypoint detection with good results. In [14], a stacked hourglass network is integrated into a fish detection pipeline that first detects instances and secondly uses a model trainied specifically on fish. [15]’s fish pose estimation system works with VGG16. Some researchers have thusly proposed fish pose analysis, but rarely are anatomical keypoints used. Therefore, a scientific niche can be attested to the specific task of anatomical landmark detection in fish, especially for measurement purposes. This project’s keypoin detection system was mostly inspired by the ideas laid down in [14], even though a different keypoint detection system is being used.\n",
    "\n",
    "### YOLO model family ###\n",
    "\n",
    "YOLO models are specifically designed to work well with real-time object detection [16], especially YOLOv7 has pushed the boundaries in how fast it can infer, according to [17], surpassing its predecessors in both speed of inference and accuracy, e.g. by 120% over YOLOv5 with a V100 inference system at an AP of 55. Wong Kin Yiu’s implementation of Wang et al. 2022 paper is based on the earlier YOLOv5 and features keypoint detection as a new feature that had formerly only been introduced by the authors of YOLO-Pose [18] as a model adaptation for human pose estimation.\n",
    "\n",
    "YOLOv7 was chosen for the task at hand due to it being the first implementation of the model family to incorporate keypoint detection, due to its high prominence in applications for real-time detection and its built-in pose estimation that can in the furure be adapted for estimating poses of fish after keypoint detection. YOLOv7 has shown good success in developing both speed and accuracy of the model family further. [17] The model family is under constant development and will likely consist of more updates to the keypoint detection mechanisms, such as YOLOv8 by Ultralytics also incorporated keypoints after YOLOv7 started. Furthermore, YOLOv7 can with relative ease be ported from still image training to video inference, therefore planting the road to real-time application in fish monitoring.\n",
    "\n",
    "As one of the most prominent family of neural architectures, the YOLO family follows some base principles, according to [19]: a single pass on network, hence the slogan \"You only look once\", a regression-based output and non-maximum suppression. The non-maximum suppression is well established in the computer vision model world as a duplicate removal technique to declutter the detected bounding boxes within an image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8abaf2-c18b-42e8-84c1-9405b9137a80",
   "metadata": {},
   "source": [
    "## Method & Implementation ##\n",
    "\n",
    "### Data Processing ###\n",
    "\n",
    "The images selected to be pipelined into the detection system are identified and a detection of fish instances occurs to crop each given image to the bounding box of the contained fis, which results in images of a rough size of ca. 500 to 800 pixels on the wider horizontal side. Images are not resized during pre-processing, but through a training image sizing parameter set to 640 pixels on the widest side for training consistency. Slight image processing, e.g. increase in brightness is done to increase image quality.\n",
    "\n",
    "The annotation process is done manually and results in MS COCO-formatted image annotations which contain bounding boxes reaching over the whole image from (0,0) to (x_max, y_max), as well as a class marker, datapoint ID, size information and, most importantly, 20 salmon-specific anatomical keypoints from head to tail. Since YOLOv7 utilizes a different annotation format from MS COCO, annotations must be converted automatically and sorted into the right folder structure for use with YOLOv7. Data augmentation is not done before processing, as the implementation of YOLOv7 consists of a Mosaic-type augmentation which create eight further variants of each image during the training process if the augmentation parameter is set to True.\n",
    "\n",
    "Pre-cropped and improved images with annotated keypoints are fed into the YOLOv7 training pipeline using YOLOv7’s built-in parameterized training execution. Variations in training parameters in YOLOv7 can include image size, variant of pretrained weights, hyperparameter files, dataset, batch size, epoch, whether an Adam optimizer or keypoint labels instead of only bounding boxes in instance detection are to be applied. Variations in hyperparameters can include initial learning rate, keypoint, class and box loss gain, IoU thresholds during training, image augmentation types and\n",
    "weight decay. The chosen parameter changes concern mainly image size, weights, hyperparameters, batch size, Adam optimizer and epochs.\n",
    "\n",
    "In YOLOv7 [17], the original training was conducted using the MS COCO 2017 dataset which contains human body annotations for pose estimation, using 17 keypoints. Since the number of 17 keypoints is hardcoded into various parts of the implementation code, the choice was made to reduce the trained fish keypoints by three and leave out the least relevant ones. Therefore, keypoints 17, 19 and 20 (starting count at 1) were omitted in training of a 17-keypoint model, as they are at this point of low relevance to deformation detection, since the area around the area prior to the caudal fin of the specimen is only considered in length estimation using keypoint #18 as its caudal fin marker.\n",
    "\n",
    "To distinguish results on varying numbers of keypoints, a 17-keypoint model and a 8-keypoint model were trained and tested. The 8-keypoint model consists of keypoints 1 through 4, which are planted around the fish mouth to detec jaw deformity, as well as keypoints 9 through 11 for width estimation and keypoint 18 for length estimation. YOLOv7 automatically saves the best weights from each model training run to be used in inference, where specific weights can be chosen to be applied for use on previously unseen data.\n",
    "\n",
    "The salmon dataset was collected at a Norwegian salmon farming facility at Smørdalen from April 2022 using an unspecified underwater mono camera setup, containing healthy specimen only. 449 images were annotated in MS COCO format with their respective bounding boxes, keypoints and keypoint\n",
    "visibilities, and delivered in PNG format. The images were cropped to bounding boxes and a fish segmentation mask including a broad border around the fish body with a blacked out background was applied. The data was split into train, test and validation sets based on the same randomization\n",
    "seed for both model trainings in a ratio of .8/.1/.1.\n",
    "\n",
    "The cod images, provided by the UFO project team, recorded in the Baltic were processed in a similar manner to the salmon images, also using a conversion on the formatted keypoint annotations and splitting into a ratio of .8/.1/.1 for train/test/validation. The set consisted of ... images of codfish in various poses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc0f11-97f1-4a2c-9cad-15f27f63fbd2",
   "metadata": {},
   "source": [
    "### Preliminary setup and initialization tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3a42cfe-ddd6-4517-98eb-c6675646b91d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from seaborn) (1.26.1)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from seaborn) (2.1.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from seaborn) (3.8.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.16.0)\n",
      "Requirement already satisfied: split-folders in c:\\users\\tnuendel\\appdata\\roaming\\python\\python311\\site-packages (0.5.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following yanked versions: 3.4.11.39, 3.4.11.41, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.5.5.62, 4.7.0.68, 4.8.0.74\n",
      "ERROR: Could not find a version that satisfies the requirement opencv-python-headless==4.1.2.30 (from versions: 3.4.10.37, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.15.55, 3.4.16.59, 3.4.17.61, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.64, 4.6.0.66, 4.7.0.72, 4.8.0.76, 4.8.1.78, 4.9.0.80, 4.10.0.82, 4.10.0.84)\n",
      "ERROR: No matching distribution found for opencv-python-headless==4.1.2.30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.22.0\n",
      "  Using cached numpy-1.22.0.zip (11.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: numpy\n",
      "  Building wheel for numpy (pyproject.toml): started\n",
      "  Building wheel for numpy (pyproject.toml): finished with status 'error'\n",
      "Failed to build numpy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for numpy (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [206 lines of output]\n",
      "  setup.py:66: RuntimeWarning: NumPy 1.22.0 may not yet support Python 3.11.\n",
      "    warnings.warn(\n",
      "  Running from numpy source directory.\n",
      "  Processing numpy/random\\_bounded_integers.pxd.in\n",
      "  Processing numpy/random\\bit_generator.pyx\n",
      "  Processing numpy/random\\mtrand.pyx\n",
      "  Processing numpy/random\\_bounded_integers.pyx.in\n",
      "  Processing numpy/random\\_common.pyx\n",
      "  Processing numpy/random\\_generator.pyx\n",
      "  Processing numpy/random\\_mt19937.pyx\n",
      "  Processing numpy/random\\_pcg64.pyx\n",
      "  Processing numpy/random\\_philox.pyx\n",
      "  Processing numpy/random\\_sfc64.pyx\n",
      "  Cythonizing sources\n",
      "  INFO: blas_opt_info:\n",
      "  INFO: blas_armpl_info:\n",
      "  INFO: No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils\n",
      "  INFO: customize MSVCCompiler\n",
      "  INFO:   libraries armpl_lp64_mp not found in ['C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\libs', 'C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\lib']\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  INFO: blas_mkl_info:\n",
      "  INFO:   libraries mkl_rt not found in ['C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\libs', 'C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\lib']\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  INFO: blis_info:\n",
      "  INFO:   libraries blis not found in ['C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\libs', 'C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\lib']\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  INFO: openblas_info:\n",
      "  INFO:   libraries openblas not found in ['C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\libs', 'C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\lib']\n",
      "  INFO: get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'\n",
      "  INFO: customize GnuFCompiler\n",
      "  WARN: Could not locate executable g77\n",
      "  WARN: Could not locate executable f77\n",
      "  INFO: customize IntelVisualFCompiler\n",
      "  WARN: Could not locate executable ifort\n",
      "  WARN: Could not locate executable ifl\n",
      "  INFO: customize AbsoftFCompiler\n",
      "  WARN: Could not locate executable f90\n",
      "  INFO: customize CompaqVisualFCompiler\n",
      "  WARN: Could not locate executable DF\n",
      "  INFO: customize IntelItaniumVisualFCompiler\n",
      "  WARN: Could not locate executable efl\n",
      "  INFO: customize Gnu95FCompiler\n",
      "  WARN: Could not locate executable gfortran\n",
      "  WARN: Could not locate executable f95\n",
      "  INFO: customize G95FCompiler\n",
      "  WARN: Could not locate executable g95\n",
      "  INFO: customize IntelEM64VisualFCompiler\n",
      "  INFO: customize IntelEM64TFCompiler\n",
      "  WARN: Could not locate executable efort\n",
      "  WARN: Could not locate executable efc\n",
      "  INFO: customize PGroupFlangCompiler\n",
      "  WARN: Could not locate executable flang\n",
      "  WARN: don't know how to compile Fortran code on platform 'nt'\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  INFO: accelerate_info:\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  INFO: atlas_3_10_blas_threads_info:\n",
      "  INFO: Setting PTATLAS=ATLAS\n",
      "  INFO:   libraries tatlas not found in ['C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\libs', 'C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\lib']\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  INFO: atlas_3_10_blas_info:\n",
      "  INFO:   libraries satlas not found in ['C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\libs', 'C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\lib']\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  INFO: atlas_blas_threads_info:\n",
      "  INFO: Setting PTATLAS=ATLAS\n",
      "  INFO:   libraries ptf77blas,ptcblas,atlas not found in ['C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\libs', 'C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\lib']\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  INFO: atlas_blas_info:\n",
      "  INFO:   libraries f77blas,cblas,atlas not found in ['C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\libs', 'C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\lib']\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  C:\\Users\\tnuendel\\AppData\\Local\\Temp\\pip-install-8k3u1u3q\\numpy_32f29076aa654ad586fa746413a21cc0\\numpy\\distutils\\system_info.py:2077: UserWarning:\n",
      "      Optimized (vendor) Blas libraries are not found.\n",
      "      Falls back to netlib Blas library which has worse performance.\n",
      "      A better performance should be easily gained by switching\n",
      "      Blas library.\n",
      "    if self._calc_info(blas):\n",
      "  INFO: blas_info:\n",
      "  INFO:   libraries blas not found in ['C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\libs', 'C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\lib']\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  C:\\Users\\tnuendel\\AppData\\Local\\Temp\\pip-install-8k3u1u3q\\numpy_32f29076aa654ad586fa746413a21cc0\\numpy\\distutils\\system_info.py:2077: UserWarning:\n",
      "      Blas (http://www.netlib.org/blas/) libraries not found.\n",
      "      Directories to search for the libraries can be specified in the\n",
      "      numpy/distutils/site.cfg file (section [blas]) or by setting\n",
      "      the BLAS environment variable.\n",
      "    if self._calc_info(blas):\n",
      "  INFO: blas_src_info:\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  C:\\Users\\tnuendel\\AppData\\Local\\Temp\\pip-install-8k3u1u3q\\numpy_32f29076aa654ad586fa746413a21cc0\\numpy\\distutils\\system_info.py:2077: UserWarning:\n",
      "      Blas (http://www.netlib.org/blas/) sources not found.\n",
      "      Directories to search for the sources can be specified in the\n",
      "      numpy/distutils/site.cfg file (section [blas_src]) or by setting\n",
      "      the BLAS_SRC environment variable.\n",
      "    if self._calc_info(blas):\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  non-existing path in 'numpy\\\\distutils': 'site.cfg'\n",
      "  INFO: lapack_opt_info:\n",
      "  INFO: lapack_armpl_info:\n",
      "  INFO:   libraries armpl_lp64_mp not found in ['C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\libs', 'C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\lib']\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  INFO: lapack_mkl_info:\n",
      "  INFO:   libraries mkl_rt not found in ['C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\libs', 'C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\lib']\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  INFO: openblas_lapack_info:\n",
      "  INFO:   libraries openblas not found in ['C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\libs', 'C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\lib']\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  INFO: openblas_clapack_info:\n",
      "  INFO:   libraries openblas,lapack not found in ['C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\libs', 'C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\lib']\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  INFO: flame_info:\n",
      "  INFO:   libraries flame not found in ['C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\libs', 'C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\lib']\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  INFO: atlas_3_10_threads_info:\n",
      "  INFO: Setting PTATLAS=ATLAS\n",
      "  INFO:   libraries tatlas,tatlas not found in C:\\Users\\tnuendel\\.conda\\envs\\FishEnv\\lib\n",
      "  INFO:   libraries tatlas,tatlas not found in C:\\\n",
      "  INFO:   libraries tatlas,tatlas not found in C:\\Users\\tnuendel\\.conda\\envs\\FishEnv\\libs\n",
      "  INFO:   libraries tatlas,tatlas not found in C:\\ProgramData\\anaconda3\\Library\\lib\n",
      "  INFO: <class 'numpy.distutils.system_info.atlas_3_10_threads_info'>\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  INFO: atlas_3_10_info:\n",
      "  INFO:   libraries satlas,satlas not found in C:\\Users\\tnuendel\\.conda\\envs\\FishEnv\\lib\n",
      "  INFO:   libraries satlas,satlas not found in C:\\\n",
      "  INFO:   libraries satlas,satlas not found in C:\\Users\\tnuendel\\.conda\\envs\\FishEnv\\libs\n",
      "  INFO:   libraries satlas,satlas not found in C:\\ProgramData\\anaconda3\\Library\\lib\n",
      "  INFO: <class 'numpy.distutils.system_info.atlas_3_10_info'>\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  INFO: atlas_threads_info:\n",
      "  INFO: Setting PTATLAS=ATLAS\n",
      "  INFO:   libraries ptf77blas,ptcblas,atlas not found in C:\\Users\\tnuendel\\.conda\\envs\\FishEnv\\lib\n",
      "  INFO:   libraries ptf77blas,ptcblas,atlas not found in C:\\\n",
      "  INFO:   libraries ptf77blas,ptcblas,atlas not found in C:\\Users\\tnuendel\\.conda\\envs\\FishEnv\\libs\n",
      "  INFO:   libraries ptf77blas,ptcblas,atlas not found in C:\\ProgramData\\anaconda3\\Library\\lib\n",
      "  INFO: <class 'numpy.distutils.system_info.atlas_threads_info'>\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  INFO: atlas_info:\n",
      "  INFO:   libraries f77blas,cblas,atlas not found in C:\\Users\\tnuendel\\.conda\\envs\\FishEnv\\lib\n",
      "  INFO:   libraries f77blas,cblas,atlas not found in C:\\\n",
      "  INFO:   libraries f77blas,cblas,atlas not found in C:\\Users\\tnuendel\\.conda\\envs\\FishEnv\\libs\n",
      "  INFO:   libraries f77blas,cblas,atlas not found in C:\\ProgramData\\anaconda3\\Library\\lib\n",
      "  INFO: <class 'numpy.distutils.system_info.atlas_info'>\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  INFO: lapack_info:\n",
      "  INFO:   libraries lapack not found in ['C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\tnuendel\\\\.conda\\\\envs\\\\FishEnv\\\\libs', 'C:\\\\ProgramData\\\\anaconda3\\\\Library\\\\lib']\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  C:\\Users\\tnuendel\\AppData\\Local\\Temp\\pip-install-8k3u1u3q\\numpy_32f29076aa654ad586fa746413a21cc0\\numpy\\distutils\\system_info.py:1902: UserWarning:\n",
      "      Lapack (http://www.netlib.org/lapack/) libraries not found.\n",
      "      Directories to search for the libraries can be specified in the\n",
      "      numpy/distutils/site.cfg file (section [lapack]) or by setting\n",
      "      the LAPACK environment variable.\n",
      "    return getattr(self, '_calc_info_{}'.format(name))()\n",
      "  INFO: lapack_src_info:\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  C:\\Users\\tnuendel\\AppData\\Local\\Temp\\pip-install-8k3u1u3q\\numpy_32f29076aa654ad586fa746413a21cc0\\numpy\\distutils\\system_info.py:1902: UserWarning:\n",
      "      Lapack (http://www.netlib.org/lapack/) sources not found.\n",
      "      Directories to search for the sources can be specified in the\n",
      "      numpy/distutils/site.cfg file (section [lapack_src]) or by setting\n",
      "      the LAPACK_SRC environment variable.\n",
      "    return getattr(self, '_calc_info_{}'.format(name))()\n",
      "  INFO:   NOT AVAILABLE\n",
      "  INFO:\n",
      "  INFO: numpy_linalg_lapack_lite:\n",
      "  INFO:   FOUND:\n",
      "  INFO:     language = c\n",
      "  INFO:     define_macros = [('HAVE_BLAS_ILP64', None), ('BLAS_SYMBOL_SUFFIX', '64_')]\n",
      "  INFO:\n",
      "  Warning: attempted relative import with no known parent package\n",
      "  C:\\Users\\tnuendel\\AppData\\Local\\Temp\\pip-build-env-q5iuj6zs\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py:275: UserWarning: Unknown distribution option: 'define_macros'\n",
      "    warnings.warn(msg)\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running config_cc\n",
      "  INFO: unifing config_cc, config, build_clib, build_ext, build commands --compiler options\n",
      "  running config_fc\n",
      "  INFO: unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options\n",
      "  running build_src\n",
      "  INFO: build_src\n",
      "  INFO: building py_modules sources\n",
      "  creating build\n",
      "  creating build\\src.win-amd64-3.11\n",
      "  creating build\\src.win-amd64-3.11\\numpy\n",
      "  creating build\\src.win-amd64-3.11\\numpy\\distutils\n",
      "  INFO: building library \"npymath\" sources\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for numpy\n",
      "ERROR: Could not build wheels for numpy, which is required to install pyproject.toml-based projects\n",
      "ERROR: Could not find a version that satisfies the requirement onnxruntime==1.10.0 (from versions: 1.15.0, 1.15.1, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.17.0, 1.17.1, 1.17.3, 1.18.0, 1.18.1, 1.19.0)\n",
      "ERROR: No matching distribution found for onnxruntime==1.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\users\\tnuendel\\appdata\\roaming\\python\\python311\\site-packages (0.15.12)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\tnuendel\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (3.1.40)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\tnuendel\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (1.34.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\tnuendel\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: pathtools in c:\\users\\tnuendel\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\tnuendel\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from wandb) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\tnuendel\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (4.25.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from Click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\tnuendel\\appdata\\roaming\\python\\python311\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tnuendel\\.conda\\envs\\fishenv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\tnuendel\\appdata\\roaming\\python\\python311\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "# ensure the following packages are present in their respective versions\n",
    "\n",
    "!pip install seaborn\n",
    "!pip install split-folders\n",
    "!pip install --force-reinstall opencv-python-headless==4.1.2.30\n",
    "!pip install --force-reinstall numpy==1.22.0\n",
    "!pip install onnxruntime==1.10.0\n",
    "!pip install wandb\n",
    "#!conda install -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31376d93-5ab3-44e8-b915-bb351b1a3683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision   import transforms\n",
    "import numpy as np\n",
    "import os\n",
    "sys.path.append(\"FishVision\")\n",
    "%matplotlib inline\n",
    "\n",
    "# check for nvidia card monitoring\n",
    "!nvidia-smi\n",
    "\n",
    "# if error with ipywidgets: use \"conda install -c conda-forge ipywidgets\" in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e35baab-5f56-4692-9260-0be8b428e0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO v7 setup\n",
    "\n",
    "%cd /home/tanjan/FishVision/yolov7_nokpt\n",
    "\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "import numpy\n",
    "\n",
    "'''\n",
    "# testing YOLO setup with inference\n",
    "!python detect.py --weights yolov7.pt --source inference/images/horses.jpg --img 640\n",
    "\n",
    "im = plt.imread('/home/tanjan/FishVision/yolov7_nokpt/runs/detect/exp5/horses.jpg')\n",
    "implot = plt.imshow(im)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee92c175-278d-49d2-ba1b-065ce90c3eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize for CUDA with pretrained pose estimation weights for human model test\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "weigths = torch.load('yolov7-w6-pose.pt', map_location=device)\n",
    "model = weigths['model']\n",
    "_ = model.float().eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.half().to(device)\n",
    "\n",
    "if model: \n",
    "    print(\"Model ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c51de87-939a-4463-90c3-94bd76d811b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "testimage = 'Pontatal-86klein.jpg'\n",
    "\n",
    "image = cv2.imread(testimage)\n",
    "image = letterbox(image, 960, stride=64, auto=True)[0]\n",
    "image_ = image.copy()\n",
    "image = transforms.ToTensor()(image)\n",
    "image = torch.tensor(np.array([image.numpy()]))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    image = image.half().to(device)   \n",
    "output, _ = model(image)\n",
    "\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0355f4b-4859-46dc-8222-2bb4c8a059cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = non_max_suppression_kpt(output, 0.25, 0.65, nc=model.yaml['nc'], nkpt=model.yaml['nkpt'], kpt_label=True)\n",
    "if output:\n",
    "    print(\"Output ready\")\n",
    "with torch.no_grad():\n",
    "    output = output_to_keypoint(output)\n",
    "nimg = image[0].permute(1, 2, 0) * 255\n",
    "nimg = nimg.cpu().numpy().astype(np.uint8)\n",
    "nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "for idx in range(output.shape[0]):\n",
    "    plot_skeleton_kpts(nimg, output[idx, 7:].T, 3)\n",
    "    \n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis('off')\n",
    "plt.imshow(nimg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a1fe4c-f22b-4108-b4e7-e7217dbffda5",
   "metadata": {},
   "source": [
    "### Application for Fish Keypoint Detection ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8aa065-292b-419a-9828-fdab63cefd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Application for Fish Keypoint Detection ##\n",
    "\n",
    "* ensure that keypoints are in YOLOv7 annotation format\n",
    "* split data into test/train\n",
    "* keep weights of yolov7-w6-pose.pt and test on fish to gain new weights from best trained model\n",
    "* infer using unseen salmon images of various types\n",
    "* no need to augment data manually, as YOLOv7 uses the mosaic technique to internally augment images in 8 extra ways per image\n",
    "\n",
    "### Visualization and choice of keypoints in 17-keypoint model ###\n",
    "\n",
    "* YOLO v7 can accomodate a maximum of 17 keypoints due to its adaptation for 17 keypoints of the human body in pose estimation, set during training with the COCO dataset\n",
    "* of 20 given keypoints (1-20), the least useful will be removed for training the model\n",
    "* keypoints 17, 19 and 20 will be removed as they're not necessary for any specific measurements\n",
    "\n",
    "### Deformity Detection ###\n",
    "\n",
    "Currently, the detected keypoints are connected within the model as if they're part of the human body. To remedy that and to see if the ratios between correctly assigned keypoints are viewed as desirable (see deformity types), distances between the following keypoints will be calculated and collected within a separate dataset to analyze statistically.\n",
    "\n",
    "* jaw deformity of keypoint ratios between KP 1 to 4: distances of 1-4, 1-2, 2-3\n",
    "* length vs. height ratio (slim fish): 1 -> 18 vs 9/10/11 in a triangle in which the midpoint of 9 and 10 is used as the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f7ac0b-bbe8-4be0-b980-9088fbf2ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example image for keypoint visualization \n",
    "\n",
    "im = plt.imread('/home/tanjan/FishVision/Smordalen_M1_2022-04-11_biomass_correct/images/fish/2022-04-11-00-01-10_right.jpg_7.png')\n",
    "kpts_20 = [(27, 48), (58, 74), (30, 61), (52, 56), (78, 28), (121, 79), (110, 98), (118, 97), (244, 21), (341, 32), (283, 124), (407, 111),\n",
    "           (445, 98), (441, 55), (470, 62), (494, 66), (515, 53), (540, 77), (521, 99), (502, 94)]\n",
    "kpts_20 = list(zip(*kpts_20))\n",
    "\n",
    "implot = plt.imshow(im)\n",
    "plt.title(\"Originally proposed 20 keypoints\")\n",
    "plt.plot(kpts_20[0],kpts_20[1], 'or')\n",
    "for x,y in zip(kpts_20[0],kpts_20[1]):\n",
    "\n",
    "    label = (kpts_20[0].index(x)+1)\n",
    "\n",
    "    plt.annotate(label, \n",
    "                 (x,y), \n",
    "                 textcoords=\"offset points\",\n",
    "                 xytext=(0,0), \n",
    "                 color='white',\n",
    "                 ha='left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 17 keypoints\n",
    "# realistically, YOLOv7 will take extra points only as zero values from the annotations, so non-used values are zeroed out\n",
    "\n",
    "kpts_17 = [(27, 48), (58, 74), (30, 61), (52, 56), (78, 28), (121, 79), (110, 98), (118, 97), (244, 21), (341, 32), (283, 124), (407, 111),\n",
    "           (445, 98), (441, 55), (470, 62), (494, 66), (0, 0), (540, 77), (0, 0), (0, 0)]\n",
    "kpts_17 = list(zip(*kpts_17))\n",
    "\n",
    "implot = plt.imshow(im)\n",
    "plt.title(\"Reduced to17 keypoints\")\n",
    "plt.plot(kpts_17[0],kpts_17[1], 'or')\n",
    "for x,y in zip(kpts_17[0],kpts_17[1]):\n",
    "    if x!=0 and y!=0:\n",
    "        label = (kpts_17[0].index(x)+1)\n",
    "        plt.annotate(label, # this is the text\n",
    "                 (x,y),\n",
    "                 textcoords=\"offset points\", \n",
    "                 xytext=(0,0),\n",
    "                 color='white',\n",
    "                 ha='left')\n",
    "plt.show()\n",
    "\n",
    "# 8 keypoints\n",
    "\n",
    "kpts_17 = [(27, 48), (58, 74), (30, 61), (52, 56), (0, 0), (0, 0), (0, 0), (0, 0), (244, 21), (341, 32), (283, 124), (0, 0),\n",
    "           (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (540, 77), (0, 0), (0, 0)]\n",
    "kpts_17 = list(zip(*kpts_17))\n",
    "\n",
    "implot = plt.imshow(im)\n",
    "plt.title(\"Reduced to 8 keypoints\")\n",
    "plt.plot(kpts_17[0],kpts_17[1], 'or')\n",
    "for x,y in zip(kpts_17[0],kpts_17[1]):\n",
    "    if x!=0 and y!=0:\n",
    "        label = (kpts_17[0].index(x)+1)\n",
    "        plt.annotate(label,\n",
    "                 (x,y),\n",
    "                 textcoords=\"offset points\",\n",
    "                 xytext=(0,0),\n",
    "                 color='white',\n",
    "                 ha='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ff1a2f-8366-40bf-b6b1-a93a71ede129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definition for COCO JSON to YOLOv7 keypoint format annotaion conversion\n",
    "# forked and adapted to keypoint formats from JSON2YOLO by Ultralytics at https://github.com/ultralytics/JSON2YOLO/\n",
    "\n",
    "import splitfolders\n",
    "import json\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def make_dirs(dir, version=0):\n",
    "    # version: labels/images folders, inside each are test/train/valid\n",
    "    if version==0:\n",
    "            # Create folders\n",
    "        dir = Path(dir)\n",
    "        for p in dir, dir / 'labels', dir / 'images':\n",
    "            p.mkdir(parents=True, exist_ok=True)  # make dir\n",
    "        return dir\n",
    "\n",
    "'''\n",
    "reordering of directories for YOLOv7 directory format\n",
    "'''\n",
    "def reorder_dirs(root):\n",
    "    # move data up a folder\n",
    "    for basedir in listdir(root): # /images and /test\n",
    "        for subdir in listdir(join(root, basedir)): # /test, /train and /val\n",
    "            for subsubdir in listdir(join(root, basedir, subdir)): # class directory (fish/anno)\n",
    "                for filename in listdir(join(root, basedir, subdir, subsubdir)): # move all files up into parent folder\n",
    "                    move(join(root, basedir, subdir, subsubdir, filename), join(root, basedir, subdir, filename))\n",
    "                rmdir(join(root, basedir, subdir, subsubdir))\n",
    "    \n",
    "    yolo_dirs = ['test', 'train', 'val']\n",
    "    for _dir in yolo_dirs:\n",
    "        print(_dir)\n",
    "        _dir = os.path.join(root, _dir)\n",
    "        if not os.path.exists(_dir):\n",
    "            os.makedirs(_dir)\n",
    "        if not os.path.exists(join(root, _dir, 'images')):\n",
    "            os.makedirs(join(root, _dir, 'images'))\n",
    "        if not os.path.exists(join(root, _dir, 'labels')):\n",
    "            os.makedirs(join(root, _dir, 'labels'))\n",
    "    \n",
    "    basedir = os.path.join(root, r'images')\n",
    "    for subdir in listdir(basedir):\n",
    "        for filename in listdir(join(root, basedir, subdir)): # move all files up into\n",
    "            move(join(root, basedir, subdir, filename), join(root, subdir, 'images', filename))\n",
    "        rmdir(join(root, basedir, subdir))\n",
    "    rmdir(join(root, basedir))\n",
    "\n",
    "    basedir = os.path.join(root, r'labels')\n",
    "    for subdir in listdir(basedir):\n",
    "        for filename in listdir(join(root, basedir, subdir)): # move all files up into\n",
    "            move(join(root, basedir, subdir, filename), join(root, subdir, 'labels', filename))\n",
    "        rmdir(join(root, basedir, subdir))    \n",
    "    rmdir(join(root, basedir))\n",
    "\n",
    "def coco91_to_coco80_class():  # converts 80-index (val2014) to 91-index (paper)\n",
    "    # https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n",
    "    # a = np.loadtxt('data/coco.names', dtype='str', delimiter='\\n')\n",
    "    # b = np.loadtxt('data/coco_paper.names', dtype='str', delimiter='\\n')\n",
    "    # x1 = [list(a[i] == b).index(True) + 1 for i in range(80)]  # darknet to coco\n",
    "    # x2 = [list(b[i] == a).index(True) if any(b[i] == a) else None for i in range(91)]  # coco to darknet\n",
    "    x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, None, 24, 25, None,\n",
    "         None, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, None, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
    "         51, 52, 53, 54, 55, 56, 57, 58, 59, None, 60, None, None, 61, None, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
    "         None, 73, 74, 75, 76, 77, 78, 79, None]\n",
    "    return x\n",
    "        \n",
    "# converter function\n",
    "def convert_coco_json(json_dir, n_kpts, omit=None, use_segments=False, cls91to80=False):\n",
    "    save_dir = json_dir\n",
    "    coco80 = coco91_to_coco80_class()\n",
    "\n",
    "    # Import json\n",
    "    for json_file in sorted(Path(json_dir).resolve().glob('*.json')):\n",
    "        print(\"enter loop\")\n",
    "        fn = Path(save_dir) / 'labels' / json_file.stem.replace('instances_', '')  # folder name\n",
    "        \n",
    "        with open(json_file) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Create image dict\n",
    "        images = {'%g' % x['id']: x for x in data['images']}\n",
    "\n",
    "        # Write labels file\n",
    "        for x in tqdm(data['annotations'], desc=f'Annotations {json_file}'):\n",
    "            img = images['%g' % x['image_id']]\n",
    "            h, w, f = img['height'], img['width'], img['file_name']\n",
    "\n",
    "            # The COCO box format is [top left x, top left y, width, height]\n",
    "            box = np.array(x['bbox'], dtype=np.float64)\n",
    "            \n",
    "            if box[0]==0 and box[1]==0:\n",
    "                # move bbox coordinates by 1 px in each inward direction to avoid faulty normalization\n",
    "                box[0] += 1\n",
    "                box[1] += 1\n",
    "                box[2] -= 1\n",
    "                box[3] -= 1\n",
    "\n",
    "            box[:2] += box[2:] / 2  # xy top-left corner to center\n",
    "            box[[0, 2]] /= w  # normalize x\n",
    "            box[[1, 3]] /= h  # normalize y\n",
    "            \n",
    "            #extract keypoints from JSON\n",
    "            keypoints = np.array(x['keypoints'], dtype=np.float64)\n",
    "            #print(\"Original KP\", keypoints)\n",
    "            \n",
    "            if omit:\n",
    "                # omit chosen keypoints\n",
    "                idx_to_omit = []\n",
    "                for kp in omit:\n",
    "                    kp -= 1\n",
    "                    idx_to_omit.append(kp*3)\n",
    "                    idx_to_omit.append(kp*3+1)\n",
    "                    idx_to_omit.append(kp*3+2)\n",
    "                keypoints = np.delete(keypoints, idx_to_omit)\n",
    "                       \n",
    "            #normalize keypoints in each triplet of x, y and occlusion flag (occlusion flag not converted)\n",
    "            keypoints[0::3] /= w # normalize x\n",
    "            keypoints[1::3] /= h # normalize y\n",
    "            print(keypoints)\n",
    "            \n",
    "            # will append three 0.000000 float values after the used keypoints and result in different indexing, but is due to data format YOLOv7 takes\n",
    "            if n_kpts < 17:\n",
    "                fillers = 17-n_kpts\n",
    "                values = [0.000000, 0.000000, 0.000000]\n",
    "                for fr in range(fillers):\n",
    "                    keypoints = np.append(keypoints, values, axis=None)\n",
    "            #print(\"Normalized KP after filling up with 0.000000\", keypoints)\n",
    "            \n",
    "            box_key = np.append(box, keypoints)\n",
    "            for val in box_key:\n",
    "                if val == 1:\n",
    "                    val = 1.0\n",
    "           \n",
    "            # Segments\n",
    "            if use_segments:\n",
    "                segments = [j for i in x['segmentation'] for j in i]  # all segments concatenated\n",
    "                s = (np.array(segments).reshape(-1, 2) / np.array([w, h])).reshape(-1).tolist()\n",
    "\n",
    "            # Write\n",
    "            if box[2] > 0 and box[3] > 0:  # if w > 0 and h > 0\n",
    "                cls = coco80[x['category_id'] - 1] if cls91to80 else x['category_id'] - 1  # class\n",
    "                line = cls, *(s if use_segments else box_key)  # cls, box/keypoints or segments\n",
    "                with open((fn / f).with_suffix('.txt'), 'a') as file:\n",
    "                    file.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "                    \n",
    "        \n",
    "print(\"Functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35fd590-c3aa-4d00-a807-3acd3a1b1b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY COMPUTE IF NECESSARY\n",
    "'''\n",
    "keypoint conversion from COCO to YOLO v7 format\n",
    "converting the ultralytics JSON2YOLO converter at https://github.com/ultralytics/JSON2YOLO for additional keypoint data\n",
    "format: class, x_center, y_center, width, height, kpt1_x, kpt1_y, visibility_1, ..., kptn_x, kptn_y, visibility_n\n",
    "'''\n",
    "\n",
    "source_dir = '/home/tanjan/FishVision/Smordalen_M1_2022-04-11_biomass_correct'\n",
    "\n",
    "# define keypoints for omission to fit the 17-keypoint format used by COCO trained YOLOv7\n",
    "# WARNING: keypoints start counting at 1, conversion will be made internally, do NOT adjust for index 0\n",
    "omitted_kp = [17, 19, 20]\n",
    "\n",
    "convert_coco_json(source_dir, 17, omitted_kp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6c67c4-95a3-41f9-b8e5-a1bfd1804c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY COMPUTE IF NECESSARY\n",
    "\n",
    "# Split with a ratio of .8 : .1 : .1 train:test:val with a reproducible seed for both labels and images created within last step\n",
    "\n",
    "from os.path import join\n",
    "from os import listdir, rmdir\n",
    "from shutil import move\n",
    "\n",
    "working_dir = '/home/tanjan/FishVision'\n",
    "\n",
    "root = os.path.join(working_dir, r'data_17')\n",
    "if not os.path.exists(root):\n",
    "    os.makedirs(root)\n",
    "\n",
    "label_dir = '/home/tanjan/FishVision/Smordalen_M1_2022-04-11_biomass_correct/labels'\n",
    "img_dir = '/home/tanjan/FishVision/Smordalen_M1_2022-04-11_biomass_correct/images'\n",
    "label_target_dir = '/home/tanjan/FishVision/data_17/labels'\n",
    "img_target_dir = '/home/tanjan/FishVision/data_17/images'\n",
    "\n",
    "splitfolders.ratio(label_dir, output=label_target_dir, seed=1337, ratio=(.8, .1, .1), group_prefix=None, move=False)\n",
    "splitfolders.ratio(img_dir, output=img_target_dir, seed=1337, ratio=(.8, .1, .1), group_prefix=None, move=False)\n",
    "\n",
    "reorder_dirs(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d088f98-ac58-4d13-bcd6-0deacbd77236",
   "metadata": {},
   "source": [
    "## Experiments ##\n",
    "\n",
    "1. setup - describe\n",
    "2. metrics\n",
    "3. training and testing with results\n",
    "4. inference on unseen fish\n",
    "\n",
    "### Metrics ###\n",
    "\n",
    "While YOLO uses the well known AP or mAP metric - meaning (mean) Average Precision in detection that works with different IoU thresholds, there are further metrics that keypoint detection systems can rely on to be judged by inference quality. The OKS (average Object Keypoint Similarity), given in percentages, quantifies the distance between keypoints that were predicted by a system and is commonly used as a measure of quality in pose estimation tasks [21]. The PCK (percentage of correct keypoints) metric similarly measures the quality of keypoint inference, but uses a threshold within which a keypoint is considered the same in spite of a distance between its predicted and true location [22]. Both measures and other are useful for quality assessement and will be used frequently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b675d777-0ac9-4c4e-a3b6-5a615ad6c736",
   "metadata": {},
   "source": [
    "### Model training and testing for 17 keypoints in salmon ###\n",
    "\n",
    "After an initial experimentation phase, the optimum values parameters to observe the 17-keypoint model training were set at a batch size of 32 and an epoch count of 200, as 200 epochs have been found sufficient to clear up most of the metric outliers and lead to a stabilized, meaningful curve,\n",
    "with an IoU threshold for AP metric calculation of 0.5. Two runs with an Adam optimizer were attempted, but resulted in strongly fluctuating mAP curves that did not stabilize. In a 600 epoch test run, mAP was found to be close to unchaning after ca. 200 epochs, while training loss still\n",
    "dropped slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df6ce0-1150-4452-81c6-d8a06f183cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize for CUDA with pretrained pose estimation weights\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "weigths = torch.load('yolov7-w6-pose.pt', map_location=device)\n",
    "model = weigths['model']\n",
    "_ = model.float().eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.half().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997335f0-bab5-440e-8437-c2943ba6f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom model training in simple CLI fashion using yolov7 pose branch without virtual torch container\n",
    "%cd /home/tanjan/FishVision/yolov7\n",
    "\n",
    "!python train.py --data data/fish_17kpts.yaml --workers 8 --epochs 50 --cfg cfg/yolov7-w6-pose.yaml --weights yolov7-w6-pose.pt --batch-size 32 --img 640 --kpt-label --sync-bn --device 0 --name yolov7-fish-seventeen --hyp data/hyp.pose.yaml\n",
    "# add \"-m torch.distributed.launch --nproc_per_node 8 --master_port 9527\" after train.py if running distributed torch containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf845a-0d52-4eff-8331-fa2a8105ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing \n",
    "\n",
    "!python test.py --data data/fish_kpts.yaml --conf 0.001 --iou 0.65 --kpt-label --weights yolov7-w6-pose.pt --batch 32 --img 640 --device 0 --name yolov7-fish-seventeen\n",
    "# -m torch.distributed.launch --nproc_per_node 8 --master_port 9527"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee87bc3-31e3-4884-8f8c-a2e26fbcf9a8",
   "metadata": {},
   "source": [
    "### Inference with best models on salmon and cod ###\n",
    "\n",
    "Unseen fish images from the test set were tested using both best models in an inference setup using YOLOv7’s built in parameterized test calls. No specific further parameters were set, except for the use of the respective best saved model weights from the best model runs. In both cases, an average of 7-8 keypoints could be infered, the PCK score over the respective three test images was ca. 57.14 % for 17 keypoints and 67.86 % for 8 keypoints, not counting uninfered keypoints that could have been recognized. The 8-keypoint model seems to perform slightly better in terms of inference on unseen images. When testing the 8-keypoint model on an unseen unhealthy fish, the keypoint inference was highly accurate with 6 found keypoints, all of which were correctly placed, yet supposedly due to jar deformation, the first two jar-based keypoints could not be infered at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b18aa-efaf-4534-9855-12ba4656353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing \n",
    "\n",
    "!python test.py --data data/fish_kpts.yaml --conf 0.001 --iou 0.65 --kpt-label --weights yolov7-w6-pose.pt --batch 32 --img 640 --device 0 --name yolov7-fish-seventeen\n",
    "# -m torch.distributed.launch --nproc_per_node 8 --master_port 9527"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea4f1e-45f1-4412-a7da-67e4ce6220d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python detect.py --weights runs/train/yolov7-fish27/weights/best.pt --source \"inf_img/4.jpg\" --kpt-label\n",
    "!python detect.py --weights runs/train/yolov7-fish27/weights/best.pt --source \"inf_img/salmosalar01.jpg\" --kpt-label\n",
    "!python detect.py --weights runs/train/yolov7-fish27/weights/best.pt --source \"inf_img/salmosalar02.jpg\" --kpt-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ebfa7-db01-477f-9856-d7b1469558b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = plt.imread('/home/tanjan/FishVision/yolov7/runs/detect/exp12/4.jpg')\n",
    "implot = plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "im = plt.imread('/home/tanjan/FishVision/yolov7/runs/detect/exp13/salmosalar01.jpg')\n",
    "implot = plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "im = plt.imread('/home/tanjan/FishVision/yolov7/runs/detect/exp14/salmosalar02.jpg')\n",
    "implot = plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b46b69-8cf1-4bd1-a5d4-f287ebe27bcf",
   "metadata": {},
   "source": [
    "## Adaptation for 8 keypoints in salmon and cod ##\n",
    "\n",
    "To achieve the best possible result with minimal effort for the given goal (deformity detection), a variant with 8 keypoints was chosen to only use those that are used for deformity detection\n",
    "\n",
    "* 1-4 for jaw deformity\n",
    "* 18 for length measurement from 1 to 18\n",
    "* 9-10 for slim fish syndrome detection\n",
    "\n",
    "The 8-keypoint model ran with the same parameters as the 17-keypoint model, namely over 200 epochs using a batch size of 32, no Adam optimizer and with an IuO threshold set at 0.5.\n",
    "\n",
    "For the given cod fish, the length is the necessary information that needs to be extracted, therefore, the length of keypoint 1 to 18 is the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa45df2-4ee1-4527-b93d-692f743ab785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY COMPUTE IF NECESSARY\n",
    "\n",
    "# create new annotations and images set in /data_8\n",
    "'''\n",
    "keypoint conversion from COCO to YOLO v7 format\n",
    "converting the ultralytics JSON2YOLO converter at https://github.com/ultralytics/JSON2YOLO for additional keypoint data\n",
    "format: class, x_center, y_center, width, height, kpt1_x, kpt1_y, visibility_1, ..., kptn_x, kptn_y, visibility_n\n",
    "'''\n",
    "source_dir = '/home/tanjan/FishVision/Smordalen_M1_2022-04-11_biomass_correct'\n",
    "\n",
    "# define keypoints for omission to fit the 17-keypoint format used by COCO trained YOLOv7\n",
    "# WARNING: keypoints start counting at 1, conversion will be made internally, do NOT adjust for index 0\n",
    "omitted_kp = [5, 6, 7, 8, 12, 13, 14, 15, 16, 17, 19, 20]\n",
    "\n",
    "convert_coco_json(source_dir, 8, omitted_kp)\n",
    "\n",
    "# Split with a ratio of .8 : .1 : .1 train:test:val with a reproducible seed for both labels and images created within last step\n",
    "\n",
    "working_dir = '/home/tanjan/FishVision'\n",
    "\n",
    "root = os.path.join(working_dir, r'data_8')\n",
    "if not os.path.exists(root):\n",
    "    os.makedirs(root)\n",
    "\n",
    "label_dir = '/home/tanjan/FishVision/Smordalen_M1_2022-04-11_biomass_correct/labels'\n",
    "img_dir = '/home/tanjan/FishVision/Smordalen_M1_2022-04-11_biomass_correct/images'\n",
    "label_target_dir = '/home/tanjan/FishVision/data_8/labels'\n",
    "img_target_dir = '/home/tanjan/FishVision/data_8/images'\n",
    "\n",
    "splitfolders.ratio(label_dir, output=label_target_dir, seed=1337, ratio=(.8, .1, .1), group_prefix=None, move=False)\n",
    "splitfolders.ratio(img_dir, output=img_target_dir, seed=1337, ratio=(.8, .1, .1), group_prefix=None, move=False)\n",
    "\n",
    "reorder_dirs(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e1c87-d897-42ce-81d2-5e169d37620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY COMPUTE IF NECESSARY\n",
    "\n",
    "# create new annotations and images set in /data_8\n",
    "'''\n",
    "keypoint conversion from COCO to YOLO v7 format\n",
    "converting the ultralytics JSON2YOLO converter at https://github.com/ultralytics/JSON2YOLO for additional keypoint data\n",
    "format: class, x_center, y_center, width, height, kpt1_x, kpt1_y, visibility_1, ..., kptn_x, kptn_y, visibility_n\n",
    "'''\n",
    "source_dir = '/home/tanjan/FishVision/Smordalen_M1_2022-04-11_biomass_correct'\n",
    "\n",
    "# define keypoints for omission to fit the 17-keypoint format used by COCO trained YOLOv7\n",
    "# WARNING: keypoints start counting at 1, conversion will be made internally, do NOT adjust for index 0\n",
    "omitted_kp = [5, 6, 7, 8, 12, 13, 14, 15, 16, 17, 19, 20]\n",
    "\n",
    "convert_coco_json(source_dir, 8, omitted_kp)\n",
    "\n",
    "# Split with a ratio of .8 : .1 : .1 train:test:val with a reproducible seed for both labels and images created within last step\n",
    "\n",
    "working_dir = '/home/tanjan/FishVision'\n",
    "\n",
    "root = os.path.join(working_dir, r'data_8')\n",
    "if not os.path.exists(root):\n",
    "    os.makedirs(root)\n",
    "\n",
    "label_dir = '/home/tanjan/FishVision/Smordalen_M1_2022-04-11_biomass_correct/labels'\n",
    "img_dir = '/home/tanjan/FishVision/Smordalen_M1_2022-04-11_biomass_correct/images'\n",
    "label_target_dir = '/home/tanjan/FishVision/data_8/labels'\n",
    "img_target_dir = '/home/tanjan/FishVision/data_8/images'\n",
    "\n",
    "splitfolders.ratio(label_dir, output=label_target_dir, seed=1337, ratio=(.8, .1, .1), group_prefix=None, move=False)\n",
    "splitfolders.ratio(img_dir, output=img_target_dir, seed=1337, ratio=(.8, .1, .1), group_prefix=None, move=False)\n",
    "\n",
    "reorder_dirs(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e59af8f-3c56-442d-bc89-082e68d48a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY COMPUTE IF NECESSARY\n",
    "\n",
    "# create new annotations and images set in /data_8\n",
    "'''\n",
    "keypoint conversion from COCO to YOLO v7 format\n",
    "converting the ultralytics JSON2YOLO converter at https://github.com/ultralytics/JSON2YOLO for additional keypoint data\n",
    "format: class, x_center, y_center, width, height, kpt1_x, kpt1_y, visibility_1, ..., kptn_x, kptn_y, visibility_n\n",
    "'''\n",
    "source_dir = '/home/tanjan/FishVision/Smordalen_M1_2022-04-11_biomass_correct'\n",
    "\n",
    "# define keypoints for omission to fit the 17-keypoint format used by COCO trained YOLOv7\n",
    "# WARNING: keypoints start counting at 1, conversion will be made internally, do NOT adjust for index 0\n",
    "omitted_kp = [5, 6, 7, 8, 12, 13, 14, 15, 16, 17, 19, 20]\n",
    "\n",
    "convert_coco_json(source_dir, 8, omitted_kp)\n",
    "\n",
    "# Split with a ratio of .8 : .1 : .1 train:test:val with a reproducible seed for both labels and images created within last step\n",
    "\n",
    "working_dir = '/home/tanjan/FishVision'\n",
    "\n",
    "root = os.path.join(working_dir, r'data_8')\n",
    "if not os.path.exists(root):\n",
    "    os.makedirs(root)\n",
    "\n",
    "label_dir = '/home/tanjan/FishVision/Smordalen_M1_2022-04-11_biomass_correct/labels'\n",
    "img_dir = '/home/tanjan/FishVision/Smordalen_M1_2022-04-11_biomass_correct/images'\n",
    "label_target_dir = '/home/tanjan/FishVision/data_8/labels'\n",
    "img_target_dir = '/home/tanjan/FishVision/data_8/images'\n",
    "\n",
    "splitfolders.ratio(label_dir, output=label_target_dir, seed=1337, ratio=(.8, .1, .1), group_prefix=None, move=False)\n",
    "splitfolders.ratio(img_dir, output=img_target_dir, seed=1337, ratio=(.8, .1, .1), group_prefix=None, move=False)\n",
    "\n",
    "reorder_dirs(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d227d-27f0-4a28-8496-ecf07665090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY COMPUTE IF NECESSARY\n",
    "\n",
    "# create new annotations and images set in /data_8\n",
    "'''\n",
    "keypoint conversion from COCO to YOLO v7 format\n",
    "converting the ultralytics JSON2YOLO converter at https://github.com/ultralytics/JSON2YOLO for additional keypoint data\n",
    "format: class, x_center, y_center, width, height, kpt1_x, kpt1_y, visibility_1, ..., kptn_x, kptn_y, visibility_n\n",
    "'''\n",
    "source_dir = '/home/tanjan/FishVision/Smordalen_M1_2022-04-11_biomass_correct'\n",
    "\n",
    "# define keypoints for omission to fit the 17-keypoint format used by COCO trained YOLOv7\n",
    "# WARNING: keypoints start counting at 1, conversion will be made internally, do NOT adjust for index 0\n",
    "omitted_kp = [5, 6, 7, 8, 12, 13, 14, 15, 16, 17, 19, 20]\n",
    "\n",
    "convert_coco_json(source_dir, 8, omitted_kp)\n",
    "\n",
    "# Split with a ratio of .8 : .1 : .1 train:test:val with a reproducible seed for both labels and images created within last step\n",
    "\n",
    "working_dir = '/home/tanjan/FishVision'\n",
    "\n",
    "root = os.path.join(working_dir, r'data_8')\n",
    "if not os.path.exists(root):\n",
    "    os.makedirs(root)\n",
    "\n",
    "label_dir = '/home/tanjan/FishVision/Smordalen_M1_2022-04-11_biomass_correct/labels'\n",
    "img_dir = '/home/tanjan/FishVision/Smordalen_M1_2022-04-11_biomass_correct/images'\n",
    "label_target_dir = '/home/tanjan/FishVision/data_8/labels'\n",
    "img_target_dir = '/home/tanjan/FishVision/data_8/images'\n",
    "\n",
    "splitfolders.ratio(label_dir, output=label_target_dir, seed=1337, ratio=(.8, .1, .1), group_prefix=None, move=False)\n",
    "splitfolders.ratio(img_dir, output=img_target_dir, seed=1337, ratio=(.8, .1, .1), group_prefix=None, move=False)\n",
    "\n",
    "reorder_dirs(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa8a29-9cae-4b7f-86df-bc5878648268",
   "metadata": {},
   "source": [
    "### Preliminary results ###\n",
    "\n",
    "Overall the training metric results for the 17- and 8-keypoint models were fairly similar with a slight favor in mAP in the best 17-keypoint model and slight variations in the metric and loss curves. Running the model test for 17 keypoints resulted in a fair amount of correctly observed keypoints.\n",
    "Some fish were left entirely undetected, other fish had more than the necessary keypoints strewn across especially the jaw area.\n",
    "\n",
    "The keypoints trained on salmon do not transfer well to the given codfish dataset, therefore a codfish-specific model should be trained with their given 8 keypoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2d2d2c-3ac7-4f73-828a-a703f1df9c01",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model training and testing for 8 keypoints on cod ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0588280a-d0b7-48e1-bea4-453f09f3c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize for CUDA with pretrained pose estimation weights\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "weigths = torch.load('yolov7-w6-pose.pt', map_location=device)\n",
    "model = weigths['model']\n",
    "_ = model.float().eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.half().to(device)\n",
    "\n",
    "# custom model training in simple CLI fashion using yolov7 pose branch without virtual torch container\n",
    "%cd /home/tanjan/FishVision/yolov7\n",
    "\n",
    "!python train.py --data data/fish_8kpts.yaml --workers 8 --epochs 200 --cfg cfg/yolov7-w6-pose.yaml --weights yolov7-w6-pose.pt --batch-size 32 --img 640 --kpt-label --sync-bn --device 0 --name yolov7-fish-eight --hyp data/hyp.pose.yaml\n",
    "# add \"-m torch.distributed.launch --nproc_per_node 8 --master_port 9527\" after train.py if running distributed torch containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff7947-2dd3-4696-affd-302f599f445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python detect.py --weights runs/train/yolov7-fish-eight/weights/best.pt --source \"inf_img/4.jpg\" --kpt-label\n",
    "!python detect.py --weights runs/train/yolov7-fish-eight/weights/best.pt --source \"inf_img/salmosalar01.jpg\" --kpt-label\n",
    "!python detect.py --weights runs/train/yolov7-fish-eight/weights/best.pt --source \"inf_img/salmosalar02.jpg\" --kpt-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267ad86-649c-4000-a3d9-e44f0e0ec219",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = plt.imread('/home/tanjan/FishVision/yolov7/runs/detect/exp15/4.jpg')\n",
    "implot = plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "im = plt.imread('/home/tanjan/FishVision/yolov7/runs/detect/exp16/salmosalar01.jpg')\n",
    "implot = plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "im = plt.imread('/home/tanjan/FishVision/yolov7/runs/detect/exp17/salmosalar02.jpg')\n",
    "implot = plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd21eec-1432-4ef4-a998-420d3366bac5",
   "metadata": {},
   "source": [
    "## Results ##\n",
    "\n",
    "### Salmon keypoint detection ###\n",
    "\n",
    "In tests within the Norwegian working group, the Mask R-CNN model used for keypoint inference performed slightly worse than the YOLOv7 models within an average range around 0.5 with the same IoU threshold of 0.5. Therefore, it might be concluded, that for this specific dataset, YOLOv7 is the\n",
    "superior model, yet this might also be due to differences in augmentation, as YOLOv7 automatically uses the mosaic augmentation technique that results in the 9-fold times of images used during model training. It might be a hint towards a need for higher image count in training datasets and more\n",
    "exploring of optimum dataset size.\n",
    "\n",
    "The current SOTA benchmark for fish keypoint detection is held by Suo et al. 2020 [14], namely set at 0.667 OKS in an 8-keypoint scenario. This is not completely comparable to the PCK metric, but moves within a similar realm. The current MS COCO keypoint detection benchmark leader [23] is 4xRSN-50 [12] and boasts a test AP of 78.6. Mask R-CNN currently ranks 15th in the COCO keypoint benchmark with a test AP of 63.1. Results are therefore consistent with typical keypoint detection algorithms and could potentially be even better than current benchmark if extra measures towards data quality, further augmentation, further hyperparameter tuning and deeper changes of the network structure. The interpretation of missing infered keypoints on unseen data can in some specific cases (4.9) be seen as the impact of deformations that should be detected, but produced unusable data to further infer on. Therefore, high regard must be given to improve direct keypoint inference in the best way possible to reduce the likelihood of missing crucial keypoints that will later be used for deformity detection and fish health status classification. In that case, parameters such as detection thresholds could be changes as to incorporate at least some less qualitative predictions to work from and possibly disregard later. Due to very few sample inferences actually containing a usable number of keypoints, the analysis of the distribution of fish measurements and ratios for classification of fish health status will thusly have to wait until the keypoint model is stable enough to predict higher numbers of keypoints to be used in measuring.\n",
    "\n",
    "### Cod keypoint detection ###\n",
    ".......\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ffa53b-ac40-4a37-9670-103e37d2ed5f",
   "metadata": {},
   "source": [
    "## Conclusion ##\n",
    "\n",
    "The experiments have overall shown that YOLOv7 lends itsself to be adapted to anatomical keypoint analysis of other species than humans. The AP and other metrics are currently below SOTA, but work well enough to attest the trained network a) a correct and b) an efficient functionality. It surpasses the project-internal current AP of ca. 0.5 in Mask R-CNN and could, with further tuning and likely a higher numbers of training images, hold up to SOTA standards.\n",
    "\n",
    "Some work remains to be done on the pipeline prior to it training specific keypoints. Especially for real-time analysis an inference, it is of the highest importance, that single fish are pre-pared for the pipeline from larger images containing a high number of fish at once. Therefore, semantic segmentation or mask-based instance segmentation are techniques that should be applied beforehand. As discussed prior, the classification of fish health status by where a single specimen’s form markers (jaw, ratios, length/width etc.) fall within a Gaussian distribution and how many standard deviations they deviate from the marker mean, will have to wait for a better inference model with a higher PCK, else the sample size would be to small to have robust results.\n",
    "\n",
    "Futhermore, stereo imaging techniqes, in which double images with slight local variation are taken by stereo camera systems, have the advantage of making it possible to measure real-world distances in images through calibration. Therefore, the introduction of stereo imaging to the project will\n",
    "result in fish measurements being able to be calculated on the fly to not only control for ratios of one fish, but also detect general sizes of specimen within the population. Real-time video inference is another task towards which this system has been building, but has not yet accomplished it yet. In further work, both the segmentation pipeline and the real-time inference could be combined to complete the proposed whole system of aquaculture monitoring to strive towards a cost-effective system for enabling better animal welfare, lower loss in fish and ultimately\n",
    "higher product output. In wildlife monitoring, the same results could be achieved for the task of estimating biomass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376ce315-5714-4b3d-9fc4-ae31e6626390",
   "metadata": {},
   "source": [
    "## References ##\n",
    "\n",
    "[1] Norwegian Seafood Industry Statistics: An Overview | Northern Delights. url: https : / /\n",
    "northerndelights.com/editorial/norwegian-seafood-industry-statistics/.\n",
    "\n",
    "[2] Fiskevelferd | Institute of Marine Research. url: https://www.hi.no/en/hi/temasider/\n",
    "aquaculture/fish-welfare.\n",
    "\n",
    "[3] Deadly fish disease discovered at Norwegian salmon farm - Fish Farmer Magazine. url:\n",
    "https://www.fishfarmermagazine.com/news/deadly- fish- disease- discovered- at-\n",
    "norwegian-salmon-farm/.\n",
    "\n",
    "[4] Disease in Fish Farms — Aquatic Life Institute. url: https://ali.fish/blog/disease-\n",
    "in-fish-farms-and-the-effects-on-fish.\n",
    "\n",
    "[5] Hamish D. Rodger, Louise Henry, and Susan O. Mitchell. “Non-infectious gill disorders of\n",
    "marine salmonid fish.” In: Reviews in Fish Biology and Fisheries 21.3 (Sept. 2011), pp. 423–\n",
    "440. issn: 09603166. doi: 10.1007/S11160-010-9182-6. url: https://www.researchgate.\n",
    "net/publication/225450608_Non- infectious_gill_disorders_of_marine_salmonid_\n",
    "fish.\n",
    "\n",
    "[6] Disturbing New Footage Shows Diseased, Deformed Salmon in B.C. Fish Farms | The Narwhal.\n",
    "url: https:/ /thenarwhal.ca/ disturbing- new- footage- shows- diseased- deformed-\n",
    "salmon-b-c-fish-farms/.\n",
    "\n",
    "[7] Hussam El Din Mohamed et al. “MSR-YOLO: Method to Enhance Fish Detection and Track-\n",
    "ing in Fish Farms.” In: Procedia Computer Science 170 (2020), pp. 539–546. issn: 18770509.\n",
    "doi: 10.1016/J.PROCS.2020.03.123.\n",
    "\n",
    "[8] Espen Stausland Kalhagen and Ørjan Langøy Olsen. “Hierarchical Fish Species Detection in\n",
    "Real-Time Video Using YOLO.” In: 70 (2020). url: https://uia.brage.unit.no/uia-\n",
    "xmlui/handle/11250/2683060.\n",
    "\n",
    "[9] Wenkai Wang, Bingwei He, and Liwei Zhang. “High-Accuracy Real-Time Fish Detection Based\n",
    "on Self-Build Dataset and RIRD-YOLOv3.” In: Complexity 2021 (2021). issn: 10990526. doi:\n",
    "10.1155/2021/4761670.\n",
    "\n",
    "[10] Chuang Yu et al. “Segmentation and measurement scheme for fish morphological features\n",
    "based on Mask R-CNN.” In: Information Processing in Agriculture 7.4 (Dec. 2020), pp. 523–\n",
    "534. issn: 2214-3173. doi: 10.1016/J.INPA.2020.01.002.\n",
    "\n",
    "[11] Abdullah Al Muksit et al. “YOLO-Fish: A robust fish detection model to detect fish in realistic\n",
    "underwater environment.” In: Ecological Informatics 72 (Dec. 2022), p. 101847. issn: 1574-\n",
    "9541. doi: 10.1016/J.ECOINF.2022.101847.\n",
    "\n",
    "[12] Learning Delicate Local Representations for Multi-Person Pose Estimation | Papers With\n",
    "Code. url: https://paperswithcode.com/paper/learning-delicate-local-representations-for.\n",
    "\n",
    "[13] Kaiming He et al. “Mask R-CNN.” In: IEEE Transactions on Pattern Analysis and Machine\n",
    "Intelligence 42.2 (Mar. 2017), pp. 386–397. issn: 19393539. doi: 10 . 1109 / TPAMI . 2018 .\n",
    "2844175. url: https://arxiv.org/abs/1703.06870v3.\n",
    "\n",
    "[14] Feiyang Suo et al. “Fish Keypoints Detection for Ecology Monitoring Based on Underwater\n",
    "Visual Intelligence.” In: 16th IEEE International Conference on Control, Automation, Robotics\n",
    "and Vision, ICARCV 2020 (Dec. 2020), pp. 542–547. doi: 10.1109/ICARCV50220.2020.\n",
    "9305424.\n",
    "\n",
    "[15] Guang Chen, Peng Sun, and Yi Shang. “Automatic fish classification system using deep learn-\n",
    "ing.” In: Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI\n",
    "2017-November (June 2018), pp. 24–29. issn: 10823409. doi: 10.1109/ICTAI.2017.00016.\n",
    "\n",
    "[16] Joseph Redmon et al. “You Only Look Once: Unified, Real-Time Object Detection.” In: Pro-\n",
    "ceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recogni-\n",
    "tion 2016-December (June 2015), pp. 779–788. issn: 10636919. doi: 10.1109/CVPR.2016.91.\n",
    "url: https://arxiv.org/abs/1506.02640v5.\n",
    "\n",
    "[17] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. “YOLOv7: Trainable bag-\n",
    "of-freebies sets new state-of-the-art for real-time object detectors.” In: (July 2022). url:\n",
    "https://arxiv.org/abs/2207.02696v1.\n",
    "\n",
    "[18] Debapriya Maji et al. “YOLO-Pose: Enhancing YOLO for Multi Person Pose Estimation Using\n",
    "Object Keypoint Similarity Loss.” In: IEEE Computer Society Conference on Computer Vision\n",
    "and Pattern Recognition Workshops 2022-June (Apr. 2022), pp. 2636–2645. issn: 21607516.\n",
    "doi: 10.1109/CVPRW56347.2022.00297. url: https://arxiv.org/abs/2204.06806v1.\n",
    "\n",
    "[19] Juan R Terven and Diana M Cordova-Esparaza. “A COMPREHENSIVE REVIEW OF YOLO:\n",
    "FROM YOLOV1 TO YOLOV8 AND BEYOND UNDER REVIEW IN ACM COMPUTING\n",
    "SURVEYS.” In: (2023).\n",
    "\n",
    "[20] Alaa Eldin Eissa. “Clinical and Laboratory Manual of Fish Diseases.” In: (). url: https:\n",
    "//www.researchgate.net/publication/301302575_Clinical_and_Laboratory_Manual_\n",
    "of_Fish_Diseases.\n",
    "\n",
    "[21] Bin Xiao, Haiping Wu, and Yichen Wei. Simple Baselines for Human Pose Estimation and\n",
    "Tracking. 2018. url: https://github..\n",
    "\n",
    "[22] Tewodros Legesse Munea et al. “The Progress of Human Pose Estimation: A Survey and\n",
    "Taxonomy of Models Applied in 2D Human Pose Estimation.” In: IEEE Access 8 (2020),\n",
    "pp. 133330–133348. issn: 21693536. doi: 10.1109/ACCESS.2020.3010248.\n",
    "\n",
    "[23] COCO Benchmark (Keypoint Detection) | Papers With Code. url: https://paperswithcode.\n",
    "com/sota/keypoint-detection-on-coco.\n",
    "15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf70d71-a23e-471a-a878-ac8a5b363870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
